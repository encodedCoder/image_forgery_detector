{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Fake Image Detection with Uniform LBP and CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "1gCg1HD-pFl6"
   },
   "source": [
    " **Real vs Fake Images Detection with LBP(Local Binary Pattern) and CNN**\n",
    "For more information, checkout this repo [https://github.com/shhotu010/FakeImageDetector](http://)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZTW-jDdcS1X"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-21T02:27:38.313279Z",
     "start_time": "2023-08-21T02:27:33.711531Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-21T02:27:38.358897Z",
     "start_time": "2023-08-21T02:27:38.344031Z"
    }
   },
   "outputs": [],
   "source": [
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-21T02:27:38.919347Z",
     "start_time": "2023-08-21T02:27:38.396039Z"
    },
    "id": "nXB89k0WpFmA",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.random.seed(2)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YdqiFx8lcS1a"
   },
   "outputs": [],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "from skimage import io\n",
    "import cv2\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TWstOMBipFmB"
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageChops, ImageEnhance\n",
    "import os\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the Path to directories\n",
    "According to the environment, if this is GoogleColab or local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authentic_images = r\"C:\\Users\\theCode\\Desktop\\img_datasets\\CASIA2\\Au\"\n",
    "tampered_images  = r\"C:\\Users\\theCode\\Desktop\\img_datasets\\CASIA2\\Tp\"\n",
    "\n",
    "# If this is google-colab then import drive\n",
    "IN_COLAB = False\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "# If this google colab then mount the Google drive and update the paths    \n",
    "if IN_COLAB:\n",
    "    drive.mount('/content/drive')\n",
    "    authentic_images = \"/content/drive/MyDrive/CASIA2_Dataset/Authentic_Images\"\n",
    "    tampered_images  = \"/content/drive/MyDrive/CASIA2_Dataset/Tampered_Images\"\n",
    "    print(\"This is GoogleColab\")\n",
    "else:\n",
    "    print(\"This is local machine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsOh1w-EcS1b"
   },
   "source": [
    "# Open a real Image and convert to LBP image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGTBt0xJpFmC"
   },
   "source": [
    "## Open a real image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "id": "lWXVkmUvpFmC",
    "outputId": "c498cb89-e0a4-4c4d-a1c1-e2f2ccb3856c",
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "real_image_path = authentic_images + \"/Au_ani_00001.jpg\"\n",
    "Image.open(real_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12ccIwbvcS1f"
   },
   "source": [
    "## Convert Real Image to LBP image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "Al4V1lJlpFmE",
    "outputId": "c1477f25-858d-473a-ad1e-d85df63a5c4c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "real_image = io.imread(real_image_path, as_gray=True)\n",
    "img_lbp = local_binary_pattern(real_image, 8, 1)\n",
    "io.imshow(img_lbp, cmap='gray')\n",
    "io.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WkI11P_ccS1h"
   },
   "source": [
    "# Open a fake Image and Convert to LBP image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGvKsuMPpFmE"
   },
   "source": [
    "## Open a fake image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "76Ut-p81pFmE",
    "outputId": "441faa1c-9787-4300-c419-bfd9bb6bb97b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fake_image_path = tampered_images + \"/Tp_D_NRN_S_N_ani10171_ani00001_12458.jpg\"\n",
    "Image.open(fake_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xN_1Mwc9cS1i"
   },
   "source": [
    "## Convert Fake Image to LBP Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jUVcoVU8lx4i"
   },
   "outputs": [],
   "source": [
    "# io.imsave(\"test.png\", img_lbp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "XNx7HpMbpFmF",
    "outputId": "4d45e951-e54a-4f7a-bce4-69351855403e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fake_image = cv2.imread(fake_image_path, 0)\n",
    "img_lbp = local_binary_pattern(fake_image, 8, 1)\n",
    "io.imshow(img_lbp, cmap='gray')\n",
    "io.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzMLeWPDpFmF"
   },
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the important parameters ****** ****** ****** **** ***** \n",
    "back to original after testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_images = 4000\n",
    "epochs = 40\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation starts here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fNcLrepcS1j"
   },
   "outputs": [],
   "source": [
    "image_size = (128, 128, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQDMV3VBpFmG"
   },
   "outputs": [],
   "source": [
    "def prepare_image(image_path):\n",
    "    image = io.imread(image_path, as_gray=True)\n",
    "    # print(image.shape)\n",
    "    image = local_binary_pattern(image, 8, 1, method='circular')\n",
    "    # print(image.shape)\n",
    "    image = np.resize(image, image_size)\n",
    "    # print(image.shape)\n",
    "    return image.flatten() / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sTQE1Foo7R5m",
    "outputId": "405e0562-652d-4e07-b074-694576d316da",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this is testing cell\n",
    "prepared_image = prepare_image(real_image_path)\n",
    "print(\"Shape of prepared image:\", prepared_image.shape)\n",
    "print(\"Dimensions of prepared image:\", prepared_image.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSEaPU7vpFmG"
   },
   "source": [
    "## Extract LBP features from Authentic Images\n",
    "Out of all authentic images, we take a certain number of random number of images for training and testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AmqtPbzEpFmG"
   },
   "outputs": [],
   "source": [
    "X = [] # ELA converted images\n",
    "Y = [] # 1 for real, 0 for fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yY-hIaxKpFmG",
    "outputId": "be2b34f4-f84b-4838-e2bb-fa2ef877dd93",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "for dirname, _, filenames in os.walk(authentic_images):\n",
    "    random.shuffle(filenames)\n",
    "    filenames = filenames[:number_of_images]\n",
    "    for filename in filenames:\n",
    "        filenameInLowerCase = filename.lower()\n",
    "        if filenameInLowerCase.endswith('jpg') or filenameInLowerCase.endswith('bmp'):\n",
    "            full_path = os.path.join(dirname, filename)\n",
    "            X.append(prepare_image(full_path))\n",
    "            Y.append(1)\n",
    "            if len(Y) % 50 == 0:\n",
    "                print(f'{len(Y)}', end=\"...   \")\n",
    "print(\"\\nTotal images processed: \", len(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract LBP features from Tampered Images\n",
    "Out of all tampered images, we take a certain number of random number of images for training and testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFHz_ygupFmH"
   },
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk(tampered_images):\n",
    "    random.shuffle(filenames)\n",
    "    filenames = filenames[:number_of_images]\n",
    "    for filename in filenames:\n",
    "        filenameInLowerCase = filename.lower()\n",
    "        if filenameInLowerCase.endswith('jpg') or filenameInLowerCase.endswith('tif'):\n",
    "            full_path = os.path.join(dirname, filename)\n",
    "            X.append(prepare_image(full_path))\n",
    "            Y.append(0)\n",
    "            if len(Y) % 50 == 0:\n",
    "                print(f'{len(Y)}', end=\"...   \")\n",
    "print(\"\\nTotal images processed: \", len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bl01l1XupFmH"
   },
   "outputs": [],
   "source": [
    "x = np.array(X)\n",
    "y = to_categorical(Y, 2)\n",
    "x = x.reshape(-1, 128, 128, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nrjYpWVepu2A"
   },
   "outputs": [],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKhBY1yWp1Nz"
   },
   "outputs": [],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHMAKNxNpFmI"
   },
   "source": [
    "# CNN Model (using k-fold cross validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense\n",
    "\n",
    "# Assuming you have X (input data) and y (target labels)\n",
    "\n",
    "# Define the number of folds (k)\n",
    "k = 5\n",
    "\n",
    "# Define your CNN model architecture\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Conv2D(filters=32,\n",
    "               kernel_size=(5, 5),\n",
    "               padding='valid',\n",
    "               activation='relu',\n",
    "               input_shape=(128, 128, 3)))\n",
    "    model.add(\n",
    "        Conv2D(filters=32,\n",
    "               kernel_size=(5, 5),\n",
    "               padding='valid',\n",
    "               activation='relu',\n",
    "               input_shape=(128, 128, 3)))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model with the appropriate loss function, optimizer, and metrics\n",
    "model = build_model()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, \n",
    "                          classes = ['fake image', 'real image'],\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph_suresh(hist):\n",
    "    fig, ax = plt.subplots(2,1)\n",
    "    ax[0].plot(hist.history['loss'], color='b', label=\"Training loss\", marker='o')\n",
    "    ax[0].plot(hist.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0], marker='o')\n",
    "    legend = ax[0].legend(loc='best', shadow=True)\n",
    "    ax[1].plot(hist.history['accuracy'], color='b', label=\"Training accuracy\", marker='o')\n",
    "    ax[1].plot(hist.history['val_accuracy'], color='r',label=\"Validation accuracy\", marker='o')\n",
    "    legend = ax[1].legend(loc='best', shadow=True)\n",
    "    ax[0].grid(True)\n",
    "    ax[1].grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_suresh(model, X_val, y_val):\n",
    "    # Predict the values from the validation dataset\n",
    "    Y_pred = model.predict(X_val)\n",
    "    # Convert predictions classes to one hot vectors \n",
    "    Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
    "    # Convert validation observations to one hot vectors\n",
    "    Y_true = np.argmax(y_val,axis = 1) \n",
    "    # compute the confusion matrix\n",
    "    confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "    # plot the confusion matrix\n",
    "    plot_confusion_matrix(confusion_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_fold_number(fold_number):\n",
    "    print(\"##############################################################################################\")\n",
    "    print(f\"#############################  This is fold {fold_number} iteration ######################################\")\n",
    "    print(\"##############################################################################################\")\n",
    "    \n",
    "def print_fold_plot_number(fold_number):\n",
    "    print(f\"**********************************************************************************************\")\n",
    "    print(f\"*************************Plots for the {fold_number} fold ************************************\")\n",
    "    print(f\"**********************************************************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize lists to store performance metrics across all folds\n",
    "accuracy_scores = []\n",
    "loss_scores = []\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "fold_number = 0\n",
    "for train_index, val_index in kf.split(x):\n",
    "    print_fold_number(fold_number)\n",
    "    # Split the data into training and validation sets\n",
    "    X_train, X_val = x[train_index], x[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    ########################################################################################################\n",
    "    optimizer = RMSprop(learning_rate=0.0005, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    early_stopping = EarlyStopping(monitor='val_acc', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "    hist = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "    ########################################################################################################\n",
    "    # Plot the loss and accuracy curves for training and validation \n",
    "    print_fold_plot_number(fold_number)\n",
    "    plot_graph_suresh(hist)\n",
    "    plot_confusion_matrix_suresh(model, X_val, y_val)\n",
    "    ########################################################################################################\n",
    "    loss, accuracy = model.evaluate(X_val, y_val)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    loss_scores.append(loss)\n",
    "    ########################################################################################################\n",
    "    fold_number += 1\n",
    "    ########################################################################################################\n",
    "\n",
    "# Calculate the average performance metrics\n",
    "avg_accuracy = np.mean(accuracy_scores)\n",
    "avg_loss = np.mean(loss_scores)\n",
    "\n",
    "# Print the average performance metrics\n",
    "print('Average Accuracy:', avg_accuracy)\n",
    "print('Average Loss:', avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_scores, loss_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/load the Model\n",
    "Load the model if already saved otherwise save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "loaded_model = None\n",
    "try:\n",
    "    loaded_model = load_model(r\"C:\\Users\\theCode\\Desktop\\LBP_\\03 - LBP_CNN\\models\\suresh3.h5\")\n",
    "    print('Loaded the saved model.')\n",
    "except:\n",
    "    model.save(r\"C:\\Users\\theCode\\Desktop\\LBP_\\03 - LBP_CNN\\models\\suresh3.h5\")\n",
    "    print('Saved the model.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs: Training Testing Loss and Accuracy Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph_suresh(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "yxpNitLLpFmL",
    "outputId": "c6d430e6-c6cc-4421-e127-dcf093357eea"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix_suresh(model, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DEvmno9pFmL"
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if loaded_model != None: model = loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5P7sosEpFmL"
   },
   "outputs": [],
   "source": [
    "class_names = ['fake', 'real']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check authenctic image prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "B2mQiR6cpFmL",
    "outputId": "516e72ca-4a4a-4122-ab48-7ded87b8d0c4"
   },
   "outputs": [],
   "source": [
    "real_image_path = authentic_images + \"/Au_ani_00002.jpg\"\n",
    "image = prepare_image(real_image_path)\n",
    "image = image.reshape(-1, 128, 128, 3)\n",
    "y_pred = model.predict(image)\n",
    "y_pred_class = np.argmax(y_pred, axis = 1)[0]\n",
    "print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check tampered image prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "Djyjv0SvpFmL",
    "outputId": "14f435e5-253e-43ce-acc1-eb84f57e8e07"
   },
   "outputs": [],
   "source": [
    "fake_image_path = tampered_images + \"/Tp_D_NRN_S_N_ani10171_ani00001_12458.jpg\"\n",
    "image = prepare_image(fake_image_path)\n",
    "image = image.reshape(-1, 128, 128, 3)\n",
    "y_pred = model.predict(image)\n",
    "y_pred_class = np.argmax(y_pred, axis = 1)[0]\n",
    "print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check tampered images predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LGvmMEyEpFmM",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check accuracy for tampered_images\n",
    "correct_t = 0\n",
    "total_t = 0\n",
    "\n",
    "for dirname, _, filenames in os.walk(tampered_images):\n",
    "    filenames = filenames[:number_of_images]\n",
    "    for filename in filenames:\n",
    "        filenameInLowerCase = filename.lower()\n",
    "        if filenameInLowerCase.endswith('jpg') or filenameInLowerCase.endswith('tif'):\n",
    "            fake_image_path = os.path.join(tampered_images, filename)\n",
    "            image = prepare_image(fake_image_path)\n",
    "            image = image.reshape(-1, 128, 128, 3)\n",
    "            y_pred = model.predict(image)\n",
    "            y_pred_class = np.argmax(y_pred, axis=1)[0]\n",
    "            total_t += 1\n",
    "            if y_pred_class == 0:\n",
    "                correct_t += 1\n",
    "                print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')\n",
    "            if total_t % 50 == 0:\n",
    "                print(f'************************ Predicted {total_t} tampered images ***************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DQa3Ct5pFmM",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy of detecting tampered images: \")\n",
    "print(f'Total: {total_t}, Correct: {correct_t}, Acc: {correct_t / total_t * 100.0}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check authentic images predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-DLoLfypFmM"
   },
   "outputs": [],
   "source": [
    "# Check accuracy for authentic_images\n",
    "correct_r = 0\n",
    "total_r = 0\n",
    "            \n",
    "for dirname, _, filenames in os.walk(authentic_images):\n",
    "    filenames = filenames[:number_of_images]\n",
    "    for filename in filenames:\n",
    "        filenameInLowerCase = filename.lower()\n",
    "        if filenameInLowerCase.endswith('jpg') or filenameInLowerCase.endswith('bmp'):\n",
    "            real_image_path = os.path.join(authentic_images, filename)\n",
    "            image = prepare_image(real_image_path)\n",
    "            image = image.reshape(-1, 128, 128, 3)\n",
    "            y_pred = model.predict(image)\n",
    "            y_pred_class = np.argmax(y_pred, axis=1)[0]\n",
    "            total_r += 1\n",
    "            if y_pred_class == 1:\n",
    "                correct_r += 1\n",
    "                print(f'Class: {class_names[y_pred_class]} Confidence: {np.amax(y_pred) * 100:0.2f}')\n",
    "            if total_r % 50 == 0:\n",
    "                print(f'*********************** Predicted {total_r} authentic images ***************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy of detecting authentic images: \")\n",
    "print(f'Total Images: {total_r}, \\nCorrect Detection: {correct_r}, \\nAcc: {correct_r / total_r * 100.0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = correct_t + correct_r\n",
    "total = total_t + total_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cuwlfSZppFmM"
   },
   "outputs": [],
   "source": [
    "print(\"Total accuracy of tampering detection: \")\n",
    "print(f'Total: {total}, Correct: {correct}, Acc: {correct / total * 100.0}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "1DEvmno9pFmL"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "612.85px",
    "left": "1166px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
